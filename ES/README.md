# 🤗 **Kit de Herramientas de Educación**

<aside>
💡 **¡Hola!** Hemos reunido un conjunto de herramientas que l@s profesor@s universitari@s pueden usar para preparar fácilmente laboratorios, tareas o clases. El contenido está diseñado de manera autónoma, de modo que se puede incorporar fácilmente al plan de estudios existente. Este contenido es gratuito y utiliza tecnologías Open Source ampliamente conocidas (`transformers`, `gradio`, etc).

Alternativamente, puede solicitar que alguien del equipo de Hugging Face ejecute los tutoriales para su clase a través de la iniciativa [Gira de demo.cratización de ML](https://www.notion.so/ML-Demo-cratization-tour-with-66847a294abd4e9785e85663f5239652)!

Además de los tutoriales, también compartimos otros recursos para profundizar en ML que pueden ayudar a diseñar el contenido del curso.

</aside>

## Nuestros Tutoriales

### 1️⃣ Un recorrido por el Hub **de** Hugging Face

> En este tutorial aprenderá a:
>
> - Explorar los más de 30,000 modelos compartidos en el Hub.
> - Implementar formas eficientes de encontrar el modelo y los datasets correctos para su propia aplicación.
> - Aprenda como incluir la colaboración en sus flujos de trabajo de ML.
>
> **_Duración: 20-40 minutos_**
>
> [Un recorrido por el Hub de Hugging Face](https://github.com/huggingface/education-toolkit/blob/main/01_tour_hub_de_huggingface.md)

### **2️⃣ Cree y aloje demos de machine learning con Gradio y Hugging Face**

> En este tutorial aprenderá a:
>
> - Explorar demos de ML creados por la comunidad.
> - Crear un demo rápido para su modelo de machine learning en Python usando la biblioteca `gradio`.
> - Alojar los demos de forma gratuita con Hugging Face Spaces.
> - Agregar su demo a su organización en Hugging Face para su clase o conferencia.
>
> **_Duración: 20-40 minutos_**
>
> 👉 [Click aquí para acceder al tutorial](https://colab.research.google.com/github/huggingface/education-toolkit/blob/main/02_ml-demos-con-gradio.ipynb)

### **3️⃣ Primeros pasos con los Transformers**

> En este tutorial aprenderá:
>
> - Sobre las redes neuronales de Transformers y cómo se pueden utilizar para abordar una amplia gama de tasks en el procesamiento del lenguaje natural y más allá.
> - El aprendizaje por transferencia para adaptar Transformers a tasks específicos.
> - Sobre la función `pipeline()` de la librería de `transformers` y cómo se puede usar para ejecutar inferencias con modelos del Hub de [Hugging Face.](https://huggingface.co/models)
>
> Este tutorial se basa en nuestro primer libro de O'Reilly _[Natural Language Processing with Transformers](https://transformersbook.com/) -_ ¡Échale un vistazo si quieres profundizar más en el tema!
>
> **_Duración: 30-45 minutos_**
>
> 👉 [click aquí para acceder al tutorial](https://colab.research.google.com/github/huggingface/education-toolkit/blob/main/03_getting-started-with-transformers.ipynb)
